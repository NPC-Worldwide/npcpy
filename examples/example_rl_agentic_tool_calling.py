# AGENTIC RL Fine Tuning with DPO - Adapted for Llama 3.2 8B SQL Tool Use

from datasets import Dataset
from datetime import datetime
import json
import glob
import os
import pandas as pd
from peft import LoraConfig
import random
import time 
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import Dict, Any, List, Optional, Callable
import re 
# Import NPC from npcpy for agent creation
from npcpy.npc_compiler import NPC
from npcpy.llm_funcs import get_llm_response # Needed for mock tool execution if not using NPC's internal get_llm_response

# --- MOCK DATABASE SCHEMA (Our data landscape!) ---
# This simulates the database structure the agent needs to understand.
MOCK_DB_SCHEMA = {
    "tables": [
        {
            "name": "SALES_DATA",
            "description": "Contains sales transactions for various products and regions.",
            "columns": [
                {"name": "TRANSACTION_ID", "type": "INTEGER", "description": "Unique identifier for each transaction."},
                {"name": "PRODUCT_NAME", "type": "TEXT", "description": "Name of the product sold."},
                {"name": "REGION", "type": "TEXT", "description": "Geographic region where the sale occurred."},
                {"name": "SALES_AMOUNT", "type": "REAL", "description": "Amount of the sale in currency."},
                {"name": "TRANSACTION_DATE", "type": "DATE", "description": "Date of the transaction (YYYY-MM-DD)."},
                {"name": "QUANTITY_SOLD", "type": "INTEGER", "description": "Number of units sold."}
            ]
        },
        {
            "name": "EMPLOYEE_PERFORMANCE",
            "description": "Tracks employee sales performance.",
            "columns": [
                {"name": "EMPLOYEE_ID", "type": "INTEGER", "description": "Unique employee identifier."},
                {"name": "EMPLOYEE_NAME", "type": "TEXT", "description": "Name of the employee."},
                {"name": "REGION", "type": "TEXT", "description": "Region where the employee works."},
                {"name": "SALES_QUARTER", "type": "TEXT", "description": "Financial quarter (e.g., Q1 2024)."},
                {"name": "QUARTERLY_SALES", "type": "REAL", "description": "Total sales generated by employee in the quarter."}
            ]
        }
    ]
}

# --- MOCK SQL QUERY EXECUTOR TOOL (Our agent's direct link to the data!) ---
# This tool simulates running an SQL query and returning results.
def sql_query_executor(sql_query: str) -> str:
    """
    Executes a given SQL query against a mock database and returns tabular results.
    Returns a JSON string with 'columns' and 'data' keys, or an 'error' key if the query is invalid.
    """
    # print(f"DEBUG: Executing mock SQL query: {sql_query}") # Uncomment for debugging tool calls
    
    sql_query_lower = sql_query.lower().strip()

    # Simulate specific responses for known queries
    if "select region, sum(sales_amount) from sales_data group by region" in sql_query_lower:
        mock_data = {
            "columns": ["REGION", "TOTAL_SALES"],
            "data": [
                ["North", 150000.00],
                ["South", 120000.00],
                ["East", 180000.00],
                ["West", 100000.00]
            ],
            "row_count": 4
        }
        return json.dumps(mock_data)
    elif "select employee_name, quarterly_sales from employee_performance where sales_quarter = 'q1 2024' order by quarterly_sales desc limit 5" in sql_query_lower:
        mock_data = {
            "columns": ["EMPLOYEE_NAME", "QUARTERLY_SALES"],
            "data": [
                ["Alice", 95000.00],
                ["Bob", 88000.00],
                ["Charlie", 72000.00],
                ["Diana", 65000.00],
                ["Eve", 60000.00]
            ],
            "row_count": 5
        }
        return json.dumps(mock_data)
    elif "select product_name, avg(sales_amount) from sales_data group by product_name order by avg(sales_amount) desc limit 3" in sql_query_lower:
        mock_data = {
            "columns": ["PRODUCT_NAME", "AVG_SALES"],
            "data": [
                ["Laptop", 1200.50],
                ["Monitor", 350.75],
                ["Keyboard", 75.20]
            ],
            "row_count": 3
        }
        return json.dumps(mock_data)
    else:
        # Generic response for other queries or simulated errors
        if "select" in sql_query_lower and "from" in sql_query_lower:
            return json.dumps({"columns": ["MESSAGE"], "data": [["Query executed successfully, no specific mock data available."]], "row_count": 1})
        else:
            return json.dumps({"error": "Invalid SQL query or syntax error in mock executor."})

TOOLS = [sql_query_executor] # Our single, powerful SQL tool!


# --- SYSTEM PROMPT TEMPLATE (The agent's instruction manual, including schema and tool definition!) ---
SYSTEM_PROMPT_TEMPLATE = f"""You are an expert Data Analyst, capable of understanding complex user queries and translating them into precise SQL.
You have access to a database with the following schema:

{json.dumps(MOCK_DB_SCHEMA, indent=2)}

You also have access to the following tool:
{{
    "name": "sql_query_executor",
    "description": "Executes a given SQL query against the database and returns tabular results.",
    "parameters": {{
        "type": "object",
        "properties": {{
            "sql_query": {{
                "type": "string",
                "description": "The SQL query to execute."
            }}
        }},
        "required": ["sql_query"]
    }}
}}

Your process should be:
1.  **Reasoning:** Analyze the user's request, identify relevant tables and columns, and formulate a step-by-step plan to construct the SQL query.
2.  **SQL Generation & Tool Call:** Generate the SQL query and call the `sql_query_executor` tool with the generated SQL.
3.  **Interpretation:** After receiving the tool's output, provide a concise and clear natural language interpretation of the results.

Always follow this exact format for your responses, including the specific tags.
When you are ready to make a tool call, ensure it is a valid JSON object on its own line after your reasoning.
When you provide your final interpretation, it should be the last message from the assistant in that turn.

<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n{json.dumps({"role": "system", "content": "You are an expert Data Analyst."})}\n<|eot_id|>
"""

class AgentToolLoop:
    """
    Manages the interaction loop for an NPC agent with tools, designed for Llama 3 chat format.
    """
    def __init__(self, agent: NPC, max_iterations: int = 8):
        self.agent = agent
        self.max_iterations = max_iterations

    def _parse_tool_call(self, text: str) -> Optional[Dict]:
        """Attempts to parse a tool call JSON object from the text."""
        try:
            # Look for a standalone JSON object that looks like a tool call
            match = re.search(r'\{\s*\"name\"\s*:\s*\"sql_query_executor\".*?\"sql_query\"\s*:\s*\".*?\"\s*\}\s*$', text, re.DOTALL)
            if match:
                return json.loads(match.group(0))
        except json.JSONDecodeError:
            pass
        return None

    def _extract_reasoning_and_tool_call(self, assistant_response: str) -> (str, Optional[Dict]):
        """Extracts reasoning and a tool call from an assistant's response."""
        reasoning = ""
        tool_call = None
        
        # Split by the tool call pattern to isolate reasoning
        tool_call_pattern = r'\{\s*\"name\"\s*:\s*\"sql_query_executor\".*?\"sql_query\"\s*:\s*\".*?\"\s*\}'
        parts = re.split(tool_call_pattern, assistant_response, maxsplit=1, flags=re.DOTALL)
        
        if len(parts) > 1:
            reasoning = parts[0].strip()
            tool_call_str = re.search(tool_call_pattern, assistant_response, flags=re.DOTALL).group(0)
            tool_call = self._parse_tool_call(tool_call_str)
        else:
            reasoning = assistant_response.strip() # No tool call found, all is reasoning
        
        # Further extract reasoning if it's wrapped in <reasoning> tags
        reasoning_match = re.search(r'<reasoning>(.*?)</reasoning>', reasoning, re.DOTALL)
        if reasoning_match:
            reasoning = reasoning_match.group(1).strip()

        return reasoning, tool_call


    def run_tool_loop(self, initial_user_prompt: str) -> Dict[str, Any]:
        """
        Runs the agent-tool interaction loop, capturing all turns.
        The initial_user_prompt should already be in Llama 3 user format.
        """
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT_TEMPLATE},
            {"role": "user", "content": initial_user_prompt.replace("<|begin_of_text|>", "").replace("<|eot_id|>", "")} # NPC expects content only
        ]
        
        raw_responses = []
        final_output = ""
        total_iterations = 0
        
        for i in range(self.max_iterations):
            total_iterations = i + 1
            # print(f"\n--- Agent Iteration {total_iterations} ---") # Debugging
            
            # The NPC's get_llm_response will handle the Llama 3 chat format internally
            response_obj = self.agent.get_llm_response(
                initial_user_prompt, # User prompt is only passed once, messages handle history
                messages=messages,
                auto_process_tool_calls=False # We want to process tool calls manually for DPO trace
            )
            
            raw_responses.append(response_obj)
            assistant_response_content = response_obj.get('response', '')
            
            # Extract reasoning and potential tool call
            reasoning, tool_call = self._extract_reasoning_and_tool_call(assistant_response_content)
            
            if tool_call:
                # print(f"DEBUG: Agent generated tool call: {tool_call}") # Debugging
                tool_output_raw = sql_query_executor(tool_call['parameters']['sql_query'])
                
                # Append assistant's response (reasoning + tool call) to messages
                messages.append({"role": "assistant", "content": assistant_response_content})
                
                # Append tool output (ipython role)
                tool_output_message = f"<|start_header_id|>ipython<|end_header_id|>\n\n{tool_output_raw}<|eot_id|>"
                messages.append({"role": "tool", "content": tool_output_message}) # Use 'tool' role for ipython output
                
                # The next turn, the agent should interpret the ipython output.
                # We need to provide a prompt that signals this.
                initial_user_prompt = f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nInterpret the results from the previous tool execution and provide your final answer.\n<|eot_id|>"
                
            else:
                # If no tool call, assume it's a final interpretation or continuation
                messages.append({"role": "assistant", "content": assistant_response_content})
                final_output = assistant_response_content # Capture the last assistant message
                
                # Check if the agent seems to be finished (e.g., provides a clear interpretation)
                if "final answer" in final_output.lower() or "summary" in final_output.lower() or "conclusion" in final_output.lower():
                    break
                
                # If not finished and no tool call, prompt to continue or make a tool call
                initial_user_prompt = f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nContinue your analysis or provide your final answer.\n<|eot_id|>"
                
        # After the loop, the final_output should contain the last assistant message
        # We need to ensure it's in the expected format for DPO training
        if not final_output and messages:
            final_output = messages[-1].get('content', '')

        return {
            "raw_responses": raw_responses,
            "final_output": final_output, # This will be the full assistant response including reasoning, tool call, and final interpretation
            "total_iterations": total_iterations,
            "messages": messages # Return full message history for trace
        }


def load_trained_model(base_model_id: str, adapter_path: Optional[str]):
    """
    Loads a model with optional LoRA adapter.
    
    Args:
        base_model_id: The base model identifier to load
        adapter_path: Optional path to LoRA adapter, if None loads base model only
        
    Returns:
        Tuple of (model, tokenizer)
    """    
    from peft import PeftModel
    print(f"Loading base model: {base_model_id}")
    model = AutoModelForCausalLM.from_pretrained(
        base_model_id,
        torch_dtype=torch.float32,
        device_map="auto",
        attn_implementation='eager',
    )
    tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    if adapter_path and os.path.exists(adapter_path):
        print(f"Loading and merging adapter: {adapter_path}")
        model = PeftModel.from_pretrained(model, adapter_path)
        model = model.merge_and_unload()
    else:
        print(f"No adapter found at {adapter_path}, using base model.")
    return model, tokenizer


def create_preference_dataset_from_traces(csv_file_path: str, min_pairs: int = 50) -> Optional[Dataset]:
    """
    Creates a DPO preference dataset from collected traces.
    It expects 'initial_user_prompt' as the prompt, and 'agent_full_response' (reasoning + tool call + interpretation)
    as chosen/rejected responses.
    """
    df = pd.read_csv(csv_file_path)
    df['reward'] = pd.to_numeric(df['reward'], errors='coerce')
    
    # Ensure all necessary columns exist and are not NaN for valid traces
    valid_df = df.dropna(subset=['reward', 'initial_user_prompt', 'agent_full_response']).copy()
    valid_df = valid_df[valid_df['reward'] > -1.0] # Filter out traces with severe errors
    
    if len(valid_df) < 2:
        print("Not enough valid traces to create preference pairs.")
        return None
    
    valid_df = valid_df.sort_values(by='reward', ascending=False)
    
    top_quantile_val = valid_df['reward'].quantile(0.8, interpolation='higher')
    low_quantile_val = valid_df['reward'].quantile(0.2, interpolation='lower')  
    
    high_reward_traces = valid_df[valid_df['reward'] >= top_quantile_val]
    low_reward_traces = valid_df[valid_df['reward'] <= low_quantile_val]
    
    min_reward_difference = 0.4
    filtered_pairs = []
    
    for _, high_trace in high_reward_traces.iterrows():
        for _, low_trace in low_reward_traces.iterrows():
            reward_gap = high_trace['reward'] - low_trace['reward']
            if reward_gap >= min_reward_difference:
                # Prompt is the initial user query in Llama 3 format
                prompt = str(high_trace['initial_user_prompt'])
                # Chosen/rejected are the full assistant responses (reasoning + tool call + interpretation)
                chosen = str(high_trace['agent_full_response'])
                rejected = str(low_trace['agent_full_response'])
                filtered_pairs.append({"prompt": prompt, "chosen": chosen, "rejected": rejected})
    
    if len(filtered_pairs) < 5:
        print(f"Only {len(filtered_pairs)} pairs with sufficient reward gap found. This may cause overfitting.")
    
    return Dataset.from_list(filtered_pairs[:min(len(filtered_pairs), 100)]) 


def train_model_with_dpo(
    csv_file_path: str,
    base_model_id: str,
    new_adapter_path: str,
    min_reward_gap: float = 0.5
):
    from trl import DPOTrainer, DPOConfig

    if not os.path.exists(csv_file_path):
        print(f"Error: CSV file not found at {csv_file_path}")
        return
    print("\n--- Starting DPO Training Process ---")
    print(f"Loading traces from {csv_file_path}...")
    preference_dataset = create_preference_dataset_from_traces(csv_file_path, min_pairs=20)
    if preference_dataset is None or len(preference_dataset) == 0:
        print("No valid preference pairs created. Cannot proceed with DPO training.")
        return

    model = AutoModelForCausalLM.from_pretrained(
        base_model_id,
        torch_dtype=torch.float32,
        device_map="auto",
        low_cpu_mem_usage=True,
    )
    tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    # Llama 3 models often expect a specific chat template for training
    # We will ensure the tokenizer uses the correct chat template for Llama 3
    tokenizer.chat_template = (
        "{% if messages[0]['role'] == 'system' %}"
        "{% set loop_messages = messages[1:] %}"
        "{% set system_message = messages[0]['content'] %}"
        "{% elif messages[0]['content'].startswith('<|begin_of_text|><|start_header_id|>system<|end_header_id|>') %}"
        "{% set loop_messages = messages[1:] %}"
        "{% set system_message = messages[0]['content'] %}"
        "{% else %}"
        "{% set loop_messages = messages %}"
        "{% set system_message = false %}"
        "{% endif %}"
        "{% for message in loop_messages %}"
        "{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}"
        "{{ raise_exception('Conversation roles must alternate user/assistant/tool. Your current message roles are: ' + messages) }}"
        "{% endif %}"
        "{% if message['role'] == 'user' %}"
        "{{ '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n' + message['content'] + '<|eot_id|>' }}"
        "{% elif message['role'] == 'assistant' %}"
        "{{ '<|start_header_id|>assistant<|end_header_id|>\n' + message['content'] + '<|eot_id|>' }}"
        "{% elif message['role'] == 'tool' %}"
        "{{ message['content'] }}" # Tool output is already in the desired format (e.g., <|start_header_id|>ipython<|end_header_id|>\n...)
        "{% endif %}"
        "{% endfor %}"
    )

    peft_config = LoraConfig(
        r=8,            
        lora_alpha=16,  
        lora_dropout=0.1,
        bias="none", 
        task_type="CAUSAL_LM",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"] 
    )
        
    training_args = DPOConfig(
        output_dir="./dpo_results",
        per_device_train_batch_size=1,
        gradient_accumulation_steps=2,
        learning_rate=1e-6,
        max_steps=50, # Increased max_steps for more training
        weight_decay=0.1,
        beta=0.5,
        logging_steps=2,
        save_steps=10,
        remove_unused_columns=False,
        max_length=1024, # Increased max_length for longer SQL queries and reasoning
        max_prompt_length=512, # Increased max_prompt_length
        dataloader_num_workers=0,
        fp16=False, bf16=False,
        optim="adamw_torch",
        warmup_steps=2,
        save_strategy="steps",
        save_total_limit=3,
    )

    trainer = DPOTrainer(
        model,
        args=training_args,
        train_dataset=preference_dataset,
        peft_config=peft_config,
        tokenizer=tokenizer, # Pass the tokenizer to DPOTrainer
        # The DPO trainer expects the prompt and response to be formatted according to the chat template
        # We will provide a formatting function to handle this.
        # This function will take a row from the dataset and return a dict with 'prompt', 'chosen', 'rejected'
        # already formatted as strings including the chat tokens.
        formatting_func=lambda x: {
            "prompt": x["prompt"],
            "chosen": x["chosen"],
            "rejected": x["rejected"]
        }
    )
    print("Starting DPO training...")
    trainer.train()
    print("DPO training complete.")
    print(f"Saving new LoRA adapter to '{new_adapter_path}'...")
    trainer.save_model(new_adapter_path)
    print("Adapter saved successfully.")


# --- REWARD FUNCTION (Our "data quality check" for agent performance!) ---
# This function evaluates how well the agent's SQL generation and interpretation performs.
def reward_fn_sql_agent(trace: Dict[str, Any]) -> float:
    """
    Calculates a reward based on the agent's SQL generation and final interpretation.
    It checks for the presence of a tool call and attempts to compare generated SQL
    and final interpretation against ground truth.
    """
    agent_full_response = trace.get("agent_full_response", "") # This is the full assistant response
    ground_truth_sql = trace['task_metadata'].get('ground_truth_sql', '').lower().strip()
    ground_truth_interpretation_keywords = [k.lower() for k in trace['task_metadata'].get('ground_truth_interpretation_keywords', [])]

    reward = 0.0

    # 1. Check for valid tool call attempt within the assistant's response
    tool_call_match = re.search(r'\{\s*\"name\"\s*:\s*\"sql_query_executor\".*?\"sql_query\"\s*:\s*\".*?\"\s*\}', agent_full_response, re.DOTALL)
    
    if tool_call_match:
        reward += 0.3 # Base reward for attempting a tool call
        try:
            tool_call_json = json.loads(tool_call_match.group(0))
            generated_sql = tool_call_json['parameters']['sql_query'].lower().strip()
            
            # 2. Compare generated SQL to ground truth
            if generated_sql == ground_truth_sql:
                reward += 0.4 # Significant reward for correct SQL
            else:
                # Partial reward if it's "close" or contains key elements (more advanced would parse SQL)
                # Check for presence of key words from ground truth SQL in generated SQL
                gt_sql_keywords = set(word for word in ground_truth_sql.split() if len(word) > 2 and word not in ["select", "from", "where", "group", "by", "order", "limit", "sum", "avg"])
                if all(keyword in generated_sql for keyword in gt_sql_keywords):
                     reward += 0.2
            
        except (json.JSONDecodeError, KeyError):
            pass # Malformed tool call, no additional reward here

    # 3. Evaluate final interpretation (after potential tool output)
    # The agent_full_response contains the *entire* assistant output for a turn.
    # We need to find the final interpretation part.
    # This assumes the interpretation comes after the tool output (if any) and is the final part of the assistant's turn.
    # We look for the last assistant block after a potential ipython block.
    
    # First, find the last assistant message content
    final_interpretation_match = re.search(r'<|start_header_id|>assistant<|end_header_id|>\n(.*?)(<|eot_id|>|$)', agent_full_response, re.DOTALL)
    if final_interpretation_match:
        final_interpretation_text = final_interpretation_match.group(1).lower().strip()
        if all(keyword in final_interpretation_text for keyword in ground_truth_interpretation_keywords):
            reward += 0.3 # Reward for a good final interpretation

    return min(1.0, reward) # Cap reward at 1.0


def collect_agent_traces_for_sql_tasks(
    tasks: List[Dict[str, Any]],
    agent: NPC, # We'll use a single agent for this example
    reward_fn: Callable[[Dict], float],
    max_iterations: int = 8
) -> List[Dict[str, Any]]:
    """
    Collects traces for SQL tasks using the AgentToolLoop.
    """
    traces = []
    
    for task_idx, task in enumerate(tasks):
        print(f"\n--- Collecting trace for Task {task_idx+1}/{len(tasks)} ---")
        initial_user_prompt = task.get('prompt', '')
        
        tool_loop = AgentToolLoop(agent, max_iterations=max_iterations)
        loop_result = tool_loop.run_tool_loop(initial_user_prompt)
        
        # The 'final_output' from run_tool_loop should be the full assistant response
        # including reasoning, tool call, and final interpretation, as formatted for Llama 3.
        agent_full_response = loop_result['final_output'] 
        
        trace = {
            "agent_name": agent.name,
            "initial_user_prompt": initial_user_prompt,
            "agent_full_response": agent_full_response,
            "total_iterations": loop_result['total_iterations'],
            "task_metadata": task['task_metadata'] # Store ground truth for reward calculation
        }
        
        trace['reward'] = reward_fn(trace)
        traces.append(trace)
        
        print(f"Task {task_idx+1} complete. Agent Reward={trace['reward']:.2f}")
    
    return traces


def run_local_agent_evaluation(
   model_id: str, # This can be base_model_id or adapter_path
   test_scenarios: List[Dict],
   model_type: str, # "baseline" or "trained"
   max_iterations: int = 8
) -> List[Dict]:
   """
   Evaluates the agent's performance on SQL test scenarios.
   """
   print(f"\n--- Running Full Local Evaluation for {model_type.upper()} Model ({model_id}) ---")
   results = []
   
   # For evaluation, we load the model and then create an NPC that uses it
   # This is where we use the 'transformers' provider for fine-tuned models
   
   # The NPC needs to be initialized with the correct model and provider for evaluation.
   # For fine-tuned models loaded via transformers, the provider is 'transformers'.
   # For base models from Ollama, it would be 'ollama'.
   
   # For simplicity in this example, we assume model_id can be passed directly to NPC
   # and it will handle loading if it's an adapter path with 'transformers' provider.
   # In a real scenario, you might load the model/tokenizer once and pass them to NPC.

   # Create a single Data Explorer Agent for evaluation
   eval_agent = NPC(
       name="data_explorer_eval",
       primary_directive=SYSTEM_PROMPT_TEMPLATE, # The system prompt is crucial!
       tools=TOOLS,
       model=model_id,
       provider="transformers" if "adapter" in model_id else "ollama" # Heuristic for provider
   )
   
   for scenario_idx, scenario in enumerate(test_scenarios):
       scenario_id = f"{scenario['scenario_id']}_{model_type}"
       print(f"--- Scenario {scenario_id} ({model_type}) ---")
       
       initial_user_prompt = scenario['initial_user_prompt'] # Already in Llama 3 user format
       
       tool_loop = AgentToolLoop(eval_agent, max_iterations=max_iterations)
       loop_result = tool_loop.run_tool_loop(initial_user_prompt)
       
       agent_full_response = loop_result['final_output']
       
       # Use the reward function to determine correctness for evaluation
       trace_for_reward = {
           "agent_full_response": agent_full_response,
           "task_metadata": {
               "ground_truth_sql": scenario['ground_truth_sql'],
               "ground_truth_interpretation_keywords": scenario['ground_truth_interpretation_keywords']
           }
       }
       reward = reward_fn_sql_agent(trace_for_reward)
       
       # For evaluation, we can define "correct" as having a reward above a certain threshold
       is_correct = 1 if reward >= 0.7 else 0 # Threshold for success
       
       results.append({
           'scenario_id': scenario_id,
           'model_type': model_type,
           'is_correct': is_correct,
           'reward': reward,
           'agent_response': agent_full_response,
           'ground_truth_sql': scenario['ground_truth_sql']
       })
       print(f"Scenario {scenario_id} complete. Reward={reward:.2f}, Correct={bool(is_correct)}")
   
   return results

def calculate_accuracy_metrics(evaluation_results: List[Dict]) -> Dict:
    """
    Calculates accuracy metrics from evaluation results.
    """
    if not evaluation_results:
        return {'accuracy': 0.0, 'total_scenarios': 0, 'correct_scenarios': 0}
    
    total_scenarios = len(evaluation_results)
    correct_scenarios = sum(res['is_correct'] for res in evaluation_results)
    accuracy = correct_scenarios / total_scenarios
    
    return {
        'accuracy': accuracy,
        'total_scenarios': total_scenarios,
        'correct_scenarios': correct_scenarios
    }


def evaluate_model_performance(
   base_model_id: str,
   adapter_path: str,
   test_scenarios_count: int = 5, # Reduced for quicker example
   max_iterations: int = 8
):
   print(f"Generating {test_scenarios_count} fresh evaluation scenarios...")
   test_scenarios = []
   eval_seed_rng = random.Random(37) 
   

   eval_data_analysis_scenarios = [
        {
            "prompt": "What are the total sales for each region?",
            "ground_truth_sql": "select region, sum(sales_amount) from sales_data group by region;",
            "ground_truth_interpretation_keywords": ["total sales", "each region"]
        },
        {
            "prompt": "Who are the top 5 employees by sales in Q1 2024?",
            "ground_truth_sql": "select employee_name, quarterly_sales from employee_performance where sales_quarter = 'q1 2024' order by quarterly_sales desc limit 5;",
            "ground_truth_interpretation_keywords": ["top 5 employees", "q1 2024 sales"]
        },
        {
            "prompt": "What is the average sales amount for the top 3 products?",
            "ground_truth_sql": "select product_name, avg(sales_amount) from sales_data group by product_name order by avg(sales_amount) desc limit 3;",
            "ground_truth_interpretation_keywords": ["average sales", "top 3 products"]
        },
        {
            "prompt": "Show me all sales transactions from 2023.",
            "ground_truth_sql": "select * from sales_data where transaction_date like '2023%';",
            "ground_truth_interpretation_keywords": ["sales transactions", "2023"]
        },
        {
            "prompt": "Which region had the highest quantity sold in any transaction?",
            "ground_truth_sql": "select region, max(quantity_sold) from sales_data group by region order by max(quantity_sold) desc limit 1;",
            "ground_truth_interpretation_keywords": ["highest quantity sold", "region"]
        }
    ]

   for i in range(test_scenarios_count):
       scenario = eval_data_analysis_scenarios[i % len(eval_data_analysis_scenarios)] # Cycle through scenarios
       
       # Format the prompt into Llama 3 user chat format
       initial_user_prompt = f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n{scenario['prompt']}\n<|eot_id|>"
       
       test_scenarios.append({
           'initial_user_prompt': initial_user_prompt,
           'ground_truth_sql': scenario['ground_truth_sql'],
           'ground_truth_interpretation_keywords': scenario['ground_truth_interpretation_keywords'],
           'scenario_id': i
       })
   
   baseline_results = run_local_agent_evaluation(base_model_id, test_scenarios, "baseline", max_iterations)
   trained_results = run_local_agent_evaluation(adapter_path, test_scenarios, "trained", max_iterations)
   
   baseline_metrics = calculate_accuracy_metrics(baseline_results)
   trained_metrics = calculate_accuracy_metrics(trained_results)
   
   print(f"\nBaseline Model Accuracy: {baseline_metrics['accuracy']:.2f}")
   print(f"Trained Model Accuracy: {trained_metrics['accuracy']:.2f}")

   improvement = trained_metrics['accuracy'] - baseline_metrics['accuracy']
   
   return {'improvement_percent': improvement, 'baseline_accuracy': baseline_metrics['accuracy'], 'trained_accuracy': trained_metrics['accuracy']}


if __name__ == "__main__":
    # --- Configuration for Llama 3.2 8B ---
    base_model = "llama3.1:8b" # Specify the Llama 3.2 8B model
    adapter_path = "./llama3.1-8b-sql-dpo-adapter" # New adapter path for this specific task

    # --- 1. Collect Traces ---
    print("\n" + "="*60)
    print("TRACE COLLECTION PHASE")
    print("="*60)

    # Define the tasks for trace collection (similar to evaluation tasks)
    trace_collection_scenarios = [
        {
            "prompt": "What are the total sales for each region?",
            "ground_truth_sql": "select region, sum(sales_amount) from sales_data group by region;",
            "ground_truth_interpretation_keywords": ["total sales", "each region"]
        },
        {
            "prompt": "Who are the top 5 employees by sales in Q1 2024?",
            "ground_truth_sql": "select employee_name, quarterly_sales from employee_performance where sales_quarter = 'q1 2024' order by quarterly_sales desc limit 5;",
            "ground_truth_interpretation_keywords": ["top 5 employees", "q1 2024 sales"]
        },
        {
            "prompt": "What is the average sales amount for the top 3 products?",
            "ground_truth_sql": "select product_name, avg(sales_amount) from sales_data group by product_name order by avg(sales_amount) desc limit 3;",
            "ground_truth_interpretation_keywords": ["average sales", "top 3 products"]
        },
        {
            "prompt": "Show me all sales transactions from 2023.",
            "ground_truth_sql": "select * from sales_data where transaction_date like '2023%';",
            "ground_truth_interpretation_keywords": ["sales transactions", "2023"]
        },
        {
            "prompt": "Which region had the highest quantity sold in any transaction?",
            "ground_truth_sql": "select region, max(quantity_sold) from sales_data group by region order by max(quantity_sold) desc limit 1;",
            "ground_truth_interpretation_keywords": ["highest quantity sold", "region"]
        }
    ]
    
    all_tasks_for_traces = []
    for i, scenario in enumerate(trace_collection_scenarios):
        initial_user_prompt_llama3 = f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n{scenario['prompt']}\n<|eot_id|>"
        all_tasks_for_traces.append({
            "prompt": initial_user_prompt_llama3,
            "task_metadata": {
                "ground_truth_sql": scenario['ground_truth_sql'],
                "ground_truth_interpretation_keywords": scenario['ground_truth_interpretation_keywords']
            }
        })
    
    # Create the Data Explorer Agent for trace collection
    trace_agent = NPC(
        name="data_explorer_trace_collector",
        primary_directive=SYSTEM_PROMPT_TEMPLATE,
        tools=TOOLS,
        model=base_model, # Use the base model to collect initial traces
        provider='ollama',
    )

    generated_traces_list = collect_agent_traces_for_sql_tasks(
        tasks=all_tasks_for_traces,
        agent=trace_agent,
        reward_fn=reward_fn_sql_agent,
        max_iterations=10 # Allow enough iterations for full SQL generation and interpretation
    )

    # Save collected traces to a CSV file
    traces_csv_file = f"llama3.2_8b_sql_traces_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    df_traces = pd.DataFrame(generated_traces_list)
    df_traces.to_csv(traces_csv_file, index=False)
    print(f"Traces saved to {traces_csv_file}")

    # --- 2. Train Model with DPO ---
    print("\n" + "="*60)
    print("DPO TRAINING PHASE")
    print("="*60)

    train_model_with_dpo(
        csv_file_path=traces_csv_file,
        base_model_id=base_model,
        new_adapter_path=adapter_path,
    )

    # --- 3. Evaluate Model Performance ---
    print("\n" + "="*60)
    print("EVALUATION PHASE")
    print("="*60)

    evaluation_results = evaluate_model_performance(
        base_model_id=base_model,
        adapter_path=adapter_path,
        test_scenarios_count=5, # Use a small number for quick evaluation
        max_iterations=10
    )
    
    print(f"\nFinal Evaluation Summary:")
    print(f"  Baseline Accuracy: {evaluation_results['baseline_accuracy']:.2f}")
    print(f"  Trained Accuracy: {evaluation_results['trained_accuracy']:.2f}")
    print(f"  Improvement: {evaluation_results['improvement_percent']:.2f}")
    print("\n" + "="*60)
    print("DPO fine-tuning and evaluation complete! *Congratulazioni!*")
    print("="*60)